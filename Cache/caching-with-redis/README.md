# Redis Embedding Cache (Rust + Azure OpenAI)

> ëª¨ë“  ë¬¸ì„œ/ì£¼ì„ì€ í•œêµ­ì–´ë¡œ ì œê³µë©ë‹ˆë‹¤.

## ê°œìš”

- ì„ë² ë”© ë²¡í„° ìºì‹±, ì‘ë‹µ ìºì‹±, ì„¸ì…˜ ëŒ€í™” ê¸°ë¡ ì €ì¥ì„ í†µí•´ Azure OpenAI ê¸°ë°˜ AI ì„œë¹„ìŠ¤ì˜ ì‘ë‹µ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒí•©ë‹ˆë‹¤.
- Redis ì¸ë©”ëª¨ë¦¬ ìºì‹œë¥¼ ì‚¬ìš©í•´ ë°˜ë³µ ì‘ì—…ì„ ì œê±°í•˜ê³ , LRU ì •ì±…ê³¼ TTLë¡œ íš¨ìœ¨ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.

## ì‚¬ìš© ê¸°ìˆ 

- Rust, Tokio(async)
- Redis (String/List) + Lua ìŠ¤í¬ë¦½íŠ¸ ì‚­ì œ
- Azure OpenAI (ì„ë² ë”©/ì±—)

## í™˜ê²½ ë³€ìˆ˜

- REDIS_URL (ê¸°ë³¸: redis://127.0.0.1:6379)
- CACHE_TTL (ê¸°ë³¸: 3600)
- REDIS_MAX_MEMORY (ê¸°ë³¸: 512mb)
- AZURE_OPENAI_ENDPOINT (ì˜ˆ: <https://YOUR_RESOURCE.openai.azure.com>)
- AZURE_OPENAI_API_KEY
- AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT (ì˜ˆ: text-embedding-3-large ë“± ë°°í¬ëª…)
- AZURE_OPENAI_CHAT_DEPLOYMENT (ì˜ˆ: gpt-4o-mini ë“± ë°°í¬ëª…)

## ë¹Œë“œ ë° ì‹¤í–‰

1) ì˜ì¡´ì„± ì„¤ì¹˜/ë¹Œë“œ

```bash
cargo build --release
```

2) Redis Docker ì¤€ë¹„(ì„ íƒ, Docker í•„ìš”)

```bash
cargo run -- setup --memory 512mb
```

3) ì—°ê²° ìƒíƒœ í™•ì¸

```bash
cargo run -- status
```

4) ë°ëª¨ ì‹¤í–‰ (ì„ë² ë”©/ì‘ë‹µ ìºì‹œ, ì„¸ì…˜ ì €ì¥)

```bash
cargo run -- test --text "ì•ˆë…•í•˜ì„¸ìš”, Redis ì„ë² ë”© ìºì‹œì…ë‹ˆë‹¤!"
```

5) í†µê³„ í™•ì¸

```bash
cargo run -- stats
```

6) ì „ì²´ ìºì‹œ ì‚­ì œ

```bash
cargo run -- clear
```

## ì£¼ìš” ëª¨ë“ˆ ê²½ë¡œ

- `src/embedding_cache.rs` ê¸°ë³¸ ì„ë² ë”© ìºì‹œ
- `src/compressed_cache.rs` ì••ì¶• ì„ë² ë”© ìºì‹œ
- `src/azure_openai.rs` Azure OpenAI REST í´ë¼ì´ì–¸íŠ¸
- `src/response_cache.rs` ì§ˆì˜-ì‘ë‹µ ìºì‹œ
- `src/session_store.rs` ì„¸ì…˜/ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì €ì¥ì†Œ
- `src/config.rs` ì„¤ì • ë¡œë”©
- `src/utils.rs` Redis Docker ì‹¤í–‰/ì—°ê²° í…ŒìŠ¤íŠ¸
- `src/main.rs` CLI ë° ë°ëª¨

## ì£¼ì˜ ì‚¬í•­

- Docker, ë„¤íŠ¸ì›Œí¬ í˜¸ì¶œì€ í™˜ê²½ì— ë”°ë¼ ì‹¤íŒ¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—ëŸ¬ ë©”ì‹œì§€ë¥¼ ì°¸ê³ í•˜ì—¬ ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”.
- Azure OpenAI í˜¸ì¶œì€ ê³¼ê¸ˆì´ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì‹¤ì œ í‚¤/ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì„¤ì •í•œ í›„ ì‚¬ìš©í•˜ì„¸ìš”.

---

# ğŸ¦€ Rust Redis ì„ë² ë”© ìºì‹œ
Redisë¥¼ í™œìš©í•œ ê³ ì„±ëŠ¥ ì„ë² ë”© ë²¡í„° ìºì‹± ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. AI/ML ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì„ë² ë”© ìƒì„± ë¹„ìš©ì„ ì ˆì•½í•˜ê³  ì‘ë‹µ ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

## âœ¨ ì£¼ìš” íŠ¹ì§•
- ë¹ ë¥¸ ì„±ëŠ¥: ë©”ëª¨ë¦¬ ê¸°ë°˜ìœ¼ë¡œ ë°€ë¦¬ì´ˆ ë‹¨ìœ„ ì‘ë‹µ
- ë©”ëª¨ë¦¬ íš¨ìœ¨: gzip ì••ì¶•ìœ¼ë¡œ 50% ì´ìƒ ë©”ëª¨ë¦¬ ì ˆì•½
- ë¹„ë™ê¸° ì§€ì›: Tokio ê¸°ë°˜ ì™„ì „ ë¹„ë™ê¸° ì²˜ë¦¬
- íƒ€ì… ì•ˆì „: Rustì˜ íƒ€ì… ì‹œìŠ¤í…œ í™œìš©
- ë°°ì¹˜ ì²˜ë¦¬: ëŒ€ëŸ‰ ë°ì´í„° íš¨ìœ¨ì  ì²˜ë¦¬
- í†µê³„ ì œê³µ: íˆíŠ¸ìœ¨ ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

## ğŸš€ ë¹ ë¥¸ ì‹œì‘
1) ìë™ ì„¤ì¹˜
```bash
curl -sSL https://raw.githubusercontent.com/your-repo/setup.sh | bash
```

2) ìˆ˜ë™ ì„¤ì¹˜
```bash
# í”„ë¡œì íŠ¸ ìƒì„±
cargo new redis-embedding-cache --bin
cd redis-embedding-cache

# ì˜ì¡´ì„± ì¶”ê°€ (Cargo.toml ì°¸ì¡°)
cargo add redis tokio anyhow serde bincode

# Redis ì„œë²„ ì‹œì‘
docker run -d -p 6379:6379 --name redis-embedding \
  redis:alpine redis-server --maxmemory 512mb --maxmemory-policy allkeys-lru
```

3) ê¸°ë³¸ ì‚¬ìš©ë²•
```rust
use caching_with_redis::EmbeddingCache;
use anyhow::Result;

#[tokio::main]
async fn main() -> Result<()> {
    // ìºì‹œ ì´ˆê¸°í™”
    let cache = EmbeddingCache::new("redis://127.0.0.1:6379", 3600).await?;

    // ì„ë² ë”© í•¨ìˆ˜ ì •ì˜
    let compute_embedding = |text: String| async move {
        // ì‹¤ì œë¡œëŠ” OpenAI APIë‚˜ ë¡œì»¬ ëª¨ë¸ í˜¸ì¶œ
        Ok(vec![0.1, 0.2, 0.3, 0.4, 0.5])
    };

    // ìºì‹œë¥¼ í†µí•œ ì„ë² ë”© ì¡°íšŒ/ê³„ì‚°
    let embedding = cache
        .get_or_compute(
            "ì•ˆë…•í•˜ì„¸ìš”, Redis ìºì‹œì…ë‹ˆë‹¤!",
            "test-model",
            compute_embedding,
        )
        .await?;

    println!("ì„ë² ë”© ì°¨ì›: {}", embedding.len());
    Ok(())
}
```

## ğŸ“Š ì„±ëŠ¥ ë¹„êµ(ì˜ˆì‹œ)
| ë°©ì‹ | ì²« í˜¸ì¶œ | ìºì‹œ íˆíŠ¸ | ì†ë„ í–¥ìƒ |
|---|---:|---:|---:|
| OpenAI API | ~200ms | ~2ms | 100x |
| ë¡œì»¬ ëª¨ë¸ | ~50ms | ~1ms | 50x |
| ì••ì¶• ìºì‹œ | ~60ms | ~3ms | 20x |

## ğŸ¯ ì‚¬ìš© ì‚¬ë¡€
### OpenAI API í†µí•©
```rust
use caching_with_redis::EmbeddingCache;
use reqwest::Client;
use serde_json::json;
use anyhow::Result;

struct OpenAIClient {
    client: Client,
    api_key: String,
    cache: EmbeddingCache,
}

impl OpenAIClient {
    async fn get_embedding_cached(&self, text: &str) -> Result<Vec<f32>> {
        let embedding_func = |text: String| async {
            // OpenAI API í˜¸ì¶œ(ì˜ˆì‹œ)
            let _resp = self.client
                .post("https://api.openai.com/v1/embeddings")
                .header("Authorization", format!("Bearer {}", self.api_key))
                .json(&json!({"model": "text-embedding-ada-002", "input": text}))
                .send()
                .await?;
            // ì‹¤ì œ ì‘ë‹µ íŒŒì‹±ì€ ìƒëµí•˜ê³ , ì˜ˆì‹œ ë²¡í„° ë°˜í™˜
            Ok::<_, anyhow::Error>(vec![0.1f32; 1536])
        };
        self.cache.get_or_compute(text, "openai-ada-002", embedding_func).await
    }
}
```

### RAG ì‹œìŠ¤í…œ
```rust
struct RAGPipeline {
    cache: caching_with_redis::EmbeddingCache,
    documents: Vec<Document>,
}

impl RAGPipeline {
    async fn search(&self, query: &str, top_k: usize) -> anyhow::Result<Vec<SearchResult>> {
        // ì¿¼ë¦¬ ì„ë² ë”© (ìºì‹œ í™œìš©)
        let query_embedding = self
            .cache
            .get_or_compute(query, "sentence-transformer", |text| async move {
                Ok::<Vec<f32>, anyhow::Error>(vec![0.0; 384])
            })
            .await?;
        // ìœ ì‚¬ë„ ê²€ìƒ‰(ì˜ˆì‹œ)
        let results = vec![]; // self.vector_search(query_embedding, top_k)
        Ok(results)
    }
}
```

### ë°°ì¹˜ ì²˜ë¦¬
```rust
async fn process_documents(
    cache: &caching_with_redis::EmbeddingCache,
    texts: Vec<String>,
) -> anyhow::Result<Vec<Vec<f32>>> {
    let batch_compute = |batch: Vec<String>| async move {
        // ë°°ì¹˜ë¡œ ì„ë² ë”© ê³„ì‚°(ì˜ˆì‹œ)
        Ok::<Vec<Vec<f32>>, anyhow::Error>(batch.into_iter().map(|_| vec![0.0f32; 384]).collect())
    };
    cache.get_or_compute_batch(&texts, "sentence-transformer", batch_compute).await
}
```

## âš™ï¸ ì„¤ì • ì˜µì…˜
### í™˜ê²½ ë³€ìˆ˜
```bash
export REDIS_URL="redis://127.0.0.1:6379"
export CACHE_TTL=3600                    # TTL (ì´ˆ)
export REDIS_MAX_MEMORY="512mb"          # ìµœëŒ€ ë©”ëª¨ë¦¬
export OPENAI_API_KEY="sk-..."           # OpenAI API í‚¤
```

### Redis ìµœì í™”
```conf
maxmemory 512mb
maxmemory-policy allkeys-lru
save ""
appendonly no
timeout 300
```

## ğŸ› ï¸ CLI ë„êµ¬
```bash
# Dockerë¡œ ë¹ ë¥´ê²Œ ìƒíƒœ í™•ì¸
pwsh ./scripts/setup.ps1 up
# ë˜ëŠ”
./scripts/setup.sh up

# ìƒíƒœ í™•ì¸
cargo run -- status

# ìºì‹œ í†µê³„
cargo run -- stats

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
cargo run -- test --text "í…ŒìŠ¤íŠ¸ ë¬¸ì¥"

# ìºì‹œ ì‚­ì œ
cargo run -- clear
```

## ğŸ“ˆ ëª¨ë‹ˆí„°ë§
```rust
let stats = cache.get_stats().await;
println!("íˆíŠ¸ìœ¨: {:.1}%", stats.hit_rate() * 100.0);
println!("ì´ í‚¤: {}", stats.total_keys);

// ì••ì¶• í†µê³„
let compression_stats = compressed_cache.get_compression_stats().await?;
println!("ì••ì¶•ë¥ : {:.2}x", compression_stats.compression_ratio);
println!("ë©”ëª¨ë¦¬ ì ˆì•½: {:.1}%", compression_stats.memory_saved_percent());
```

## ğŸ³ Docker ë°°í¬
í˜„ì¬ ë¦¬í¬ì§€í† ë¦¬ì—ëŠ” `Dockerfile`, `docker-compose.yml`ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
```yaml
version: '3.8'
services:
  redis:
    image: redis:7-alpine
    command: >
      redis-server --maxmemory 512mb --maxmemory-policy allkeys-lru --save "" --appendonly no --timeout 300 --tcp-keepalive 60
    ports: ["6379:6379"]
    volumes: ["redis_data:/data"]
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  embedding-cache:
    build: .
    image: caching_with_redis:latest
    depends_on:
      redis:
        condition: service_healthy
    environment:
      - REDIS_URL=redis://redis:6379
      - CACHE_TTL=3600
      - RUST_LOG=info
    restart: unless-stopped
    command: ["status"]

  redis-commander:
    image: rediscommander/redis-commander:latest
    environment:
      - REDIS_HOSTS=local:redis:6379
    ports: ["8081:8081"]

volumes:
  redis_data: { driver: local }
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë° ì˜ˆì œ ì‹¤í–‰
```bash
# ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
cargo test

# ì˜ˆì œ ì‹¤í–‰
cargo run --example openai_integration
cargo run --example rag_pipeline
cargo run --example performance_comparison
```

## ğŸ¤ ê¸°ì—¬í•˜ê¸°
1) Fork the project
2) Create your feature branch (git checkout -b feature/amazing-feature)
3) Commit your changes (git commit -m 'Add some amazing feature')
4) Push to the branch (git push origin feature/amazing-feature)
5) Open a Pull Request

## ğŸ“„ ë¼ì´ì„ ìŠ¤
ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ìˆìŠµë‹ˆë‹¤. LICENSE íŒŒì¼ì„ ì°¸ì¡°í•˜ì„¸ìš”.

## ğŸ”— ê´€ë ¨ ë§í¬
- Redis ê³µì‹ ë¬¸ì„œ
- Tokio ë¹„ë™ê¸° ëŸ°íƒ€ì„
- OpenAI API ë¬¸ì„œ
- Sentence Transformers

## â“ FAQ
Q: ì–´ë–¤ ì„ë² ë”© ëª¨ë¸ì„ ì§€ì›í•˜ë‚˜ìš”?
A: ëª¨ë“  ì„ë² ë”© ëª¨ë¸ì„ ì§€ì›í•©ë‹ˆë‹¤. ë°˜í™˜ê°’ì´ Vec<f32> í˜•íƒœë©´ ìºì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Q: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì€ ì–´ëŠ ì •ë„ì¸ê°€ìš”?
A: OpenAI ì„ë² ë”© ê¸°ì¤€ìœ¼ë¡œ 1000ê°œë‹¹ ì•½ 6MB (ì••ì¶• ì‹œ ë” ì ˆì•½) ì‚¬ìš©í•©ë‹ˆë‹¤.

Q: í”„ë¡œë•ì…˜ì—ì„œ ì‚¬ìš©í•´ë„ ë˜ë‚˜ìš”?
A: ë„¤, íƒ€ì… ì•ˆì „ì„±ê³¼ ì—ëŸ¬ ì²˜ë¦¬ê°€ ì˜ ë˜ì–´ìˆì–´ í”„ë¡œë•ì…˜ í™˜ê²½ì— ì í•©í•©ë‹ˆë‹¤.

Q: ë‹¤ë¥¸ ì–¸ì–´ ë°”ì¸ë”©ì´ ìˆë‚˜ìš”?
A: í˜„ì¬ëŠ” Rustë§Œ ì§€ì›í•˜ì§€ë§Œ, FFIë¥¼ í†µí•´ ë‹¤ë¥¸ ì–¸ì–´ì—ì„œë„ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.
